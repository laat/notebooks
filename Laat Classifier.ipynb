{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from sklearn import datasets\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from sklearn.datasets.base import Bunch\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Throwing out a set of theorems and definitions\n",
    "## Bayes theorem:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|\\vec{x}) = \\frac{P(y)P(\\vec{x}|y)}{P(\\vec{x})}\n",
    "\\end{align}\n",
    "\n",
    "## Mean Squared Error\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname {MSE}={\\frac  {1}{n}}\\sum _{ {i=1} }^{n}({\\hat  {Y_{i}}}-Y_{i})^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier:\n",
    "\\begin{equation}\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}{P(x_1, \\dots, x_n)} \\\\\n",
    "\\end{equation}\n",
    "Using the naive independence assumption that\n",
    "\\begin{equation}\n",
    "P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)\n",
    "\\end{equation}\n",
    "simplify\n",
    "\\begin{equation}\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}\n",
    "\\end{equation}\n",
    "\n",
    "${P(x_1, \\dots, x_n)}$ is constant given the input.\n",
    "\n",
    "\\begin{align}\n",
    "P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y) \\\\\n",
    "\\Downarrow \\\\\n",
    "\\hat{y} = \\underset{y}{\\operatorname{argmax}}\\ P(y) \\prod_{i=1}^{n} P(x_i \\mid y) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Because float underflow, $log$ trick is often used\n",
    "\\begin{align}\n",
    "\\underset{y}{\\operatorname{argmax}}\\ P(y) \\prod_{i=1}^{n} P(x_i \\mid y)  & \\propto \\underset{y}{\\operatorname{argmax}}\\  log(P(y)) + \\sum _{ {i=1} }^{n} log(P(x_{i}\\vert y)) \\\\\n",
    " & = \\underset{y}{\\operatorname{argmax}}\\ - (log(P(y)) + \\sum _{ {i=1} }^{n} log(P(x_{i}\\vert y))) \\\\\n",
    " & = \\underset{y}{\\operatorname{argmin}}\\ - log(P(y)) - \\sum _{ {i=1} }^{n} log(P(x_{i}\\vert y)))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def euclid(x, y):\n",
    "    return np.linalg.norm(x-y)\n",
    "euclid(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import six\n",
    "\n",
    "class MVKNN(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "    def __init__(self, distance_function=euclid, k=5):\n",
    "        self.distance_function = distance_function\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "    \n",
    "    def predict(self, tests):\n",
    "        results = []\n",
    "        for t in tests:\n",
    "            distances = np.apply_along_axis(self.distance_function, 1, self.data, t)\n",
    "            idx = np.argpartition(distances, self.k)\n",
    "            closest_classes = self.target[idx[:self.k]]\n",
    "            cl = np.bincount(closest_classes).argmax()\n",
    "            results.append(cl)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Mean Classification Rule ($k$MC) for $k$NN:\n",
    "\n",
    "General\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &=\\underset{y}{\\operatorname{argmin}}\\ \\frac{\\sum_{i=1}^k \\text{dist}(\\vec{x}, m_{iy})}{k}\\\\\n",
    "\\end{align}\n",
    "k is independent of y, simplify\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &=\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\text{dist}(\\vec{x}, m_{iy})\\\\\n",
    "\\end{align}\n",
    "\n",
    "Manhattan\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &=\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|}\n",
    "\\end{align}\n",
    "\n",
    "With euclids distance\n",
    "\\begin{align}\n",
    "{\\hat  {y}}  &=\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(x_i - m_{ijy})^2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KMC(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "    def __init__(self, distance_function=euclid, k=5):\n",
    "        self.distance_function = distance_function\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "    \n",
    "    def predict(self, tests):\n",
    "        results = []\n",
    "        for t in tests:\n",
    "            all_distances = np.apply_along_axis(self.distance_function, 1, self.data, t)\n",
    "            target = np.unique(self.target)\n",
    "            distances = []\n",
    "            for y in target:\n",
    "                of_y = np.transpose(np.argwhere(self.target == y))[0]\n",
    "                idx = np.argpartition(all_distances[of_y], self.k)\n",
    "                dist_sum = np.sum(all_distances[of_y][idx[:self.k]])\n",
    "                distances.append(dist_sum)\n",
    "            cl = np.array(target)[np.argmin(distances)]\n",
    "            results.append(cl)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Laat Classifier\n",
    "\n",
    "I fantasize that $k$NN and NB are exactly the same. Can the classification rules be combined in an intuitive way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For funzies \n",
    "\n",
    "Consider Naive Bayes, $k$MC with manhattan and euclids distance:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ P(y) \\prod _{ {i=1} }^{n}P(x_{i}\\vert y) \\\\\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|} \\\\\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(x_i - m_{ijy})^2}}\n",
    "\\end{align}\n",
    "\n",
    "Observation:\n",
    "\n",
    "$k$NN does not have something that looks like $P(y)$, could the \"ball\" arround $\\vec{x}$ defined by $k$ be somewhat similar? \n",
    "\n",
    "If so, then \n",
    "\\begin{equation} \n",
    "\\prod _{ {i=1} }^{n}P(x_{i}\\vert y)\n",
    "\\end{equation}\n",
    "from naive bayes seems more interesting when combining the two concepts.\n",
    "\n",
    "\n",
    "If its not, then maybe using $P(y)$ in $k$NN somehow could yield better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using $P(y)$ in $k$NN\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ (1-P(y)) \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(x_i - m_{ijh})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "My gut feeling says that this probably has too much impact on estimation, and we would end up with classifying too many $\\hat{y}$ with the highest $P(y)$ especially in cases where differences in $P(y)$ is high "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PYKMC(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "    def __init__(self, distance_function=euclid, k=5):\n",
    "        self.distance_function = distance_function\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "    \n",
    "    def prob(self, y):\n",
    "        return len(np.argwhere(self.target == y))/len(self.target)\n",
    "    \n",
    "    def predict(self, tests):\n",
    "        results = []\n",
    "        for t in tests:\n",
    "            all_distances = np.apply_along_axis(self.distance_function, 1, self.data, t)\n",
    "            target = np.unique(self.target)\n",
    "            distances = []\n",
    "            for y in target:\n",
    "                of_y = np.transpose(np.argwhere(self.target == y))[0]\n",
    "                idx = np.argpartition(all_distances[of_y], self.k)\n",
    "                dist_sum = np.sum(all_distances[of_y][idx[:self.k]])\n",
    "                distances.append((1-self.prob(y)) * dist_sum)\n",
    "            cl = np.array(target)[np.argmin(distances)]\n",
    "            results.append(cl)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### results\n",
    "\n",
    "no effect on breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtracting likelihood from distance\n",
    "Just subtracting the likelyhood from the distance, would that yield better classification?\n",
    "Seems silly. \n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} =\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|} - P(x_{i}\\vert y)\n",
    "\\end{equation}\n",
    "\n",
    "Wait, negative distances is probably bad. Lets fix that....\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} =\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|} + (1-P(x_{i}\\vert y))\n",
    "\\end{equation}\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using $P(y)$ and subtracting likelihood from distance\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} =\\underset{y}{\\operatorname{argmin}}\\ (1-P(y)) \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|}+ (1-P(x_{i}\\vert y))\n",
    "\\end{equation}\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding imaginary likelyhood to $m_{ijy}$?\n",
    "\n",
    "Mapping $x$ to imaginary $x$-likelyhood:\n",
    "\\begin{equation}\n",
    "\\mathcal{I}_{y}(x) = x + P(x|y)i\n",
    "\\end{equation}\n",
    "\n",
    "Then\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(x_i - \\mathcal{I}_{y}(m_{ijy}))^2}}\n",
    "\\end{equation}\n",
    "\n",
    "Seems to me that this could \"clean\" the training data, unlikely features of $m_{iy}$ gives logner distances from the test vector $\\vec{x}$. It could even be computed once when training.\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding imaginary likelyhood to $x_i$?\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}_{y}(x_i) - m_{ijy})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "This should penalize features in $\\vec{x}$ that is unlikely to be a part of $y$\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding imaginary likelyhood to $x_i$ and $m_{ijy}$?\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}_{y}(x_i) - \\mathcal{I}_{y}(m_{ijy}))^2}}\n",
    "\\end{equation}\n",
    "\n",
    "Nah, this sort of negates the penalty for unlikely features in $y$. We can do better!\n",
    "\n",
    "Lets take the complex conjugate $\\overline{\\mathcal{I}( \\cdot, \\cdot )}$ of one of them, it shouldn't matter which.\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}_{y}(x_i) - \\overline {\\mathcal{I}_{y}(m_{ijy})})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "This should penalize both unlikely features in $\\vec{x}$ and pushing noisy training data away from $\\vec{x}$\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using $P(y)$ *and* imaginary likelyhood\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ (1-P(y)) \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}_{y}(x_i) - \\overline {\\mathcal{I}_{y}(m_{ijh})})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is the distance a metric?\n",
    "\\begin{equation}\n",
    "d_{y}(\\vec{x}, \\vec{y}) = \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}_{y}(x_i) - \\overline {\\mathcal{I}_{y}(y_i)})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "1. $d_{y}(x,y)\\geq 0$ non-negativity üëç\n",
    "2. $d_{y}(x,x) = 0$ identity üëé\n",
    "3. $d_{y}(x,y) = d_{y}(y,x) $ symmetry üëç\n",
    "4. $d_{y}(x,z) \\leq d_{y}(x,y) + d_{y}(y,z)$  triangle inequality ? üëé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE as distance?\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname {MSE}={\\frac  {1}{n}}\\sum _{ {i=1} }^{n}({\\hat  {Y_{i}}}-Y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k {\\frac  {1}{n}}\\sum _{ {i=1} }^{n}({x_{i}}-m_{ijy})^{2} \\\\\n",
    "&=\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum _{ {i=1} }^{n}({x_{i}}-m_{ijy})^{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis distance\n",
    "\n",
    "generalized squared interpoint distance, $S$ is the covariance matrix\n",
    "\n",
    "\\begin{equation}\n",
    "madi(\\vec{x},\\vec{y})=\\sqrt{(\\vec{x}-\\vec{y})^T S^{-1} (\\vec{x}-\\vec{y})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{(\\vec{x}-\\vec{y})^T S^{-1} (\\vec{x}-\\vec{y})} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mse_dist(x, y):\n",
    "    return np.sum((x-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Funzies\n",
    "\n",
    "### Requirement: \n",
    "\n",
    "All features must be normalized so that all $x$ in $X$ is between $[0,1]$\n",
    "\n",
    "### consider:\n",
    "\n",
    "Generalized Mean:\n",
    "\\begin{align}\n",
    "\\mathrm {m}_p(\\vec{x})=\\left({\\frac  {1}{n}}\\sum _{ {i=1} }^{n}x_{i}^{p}\\right)^{ { {\\frac  {1}{p}}}}\n",
    "\\end{align}\n",
    "\n",
    "Generalized Mean Distance:\n",
    "\\begin{align}\n",
    "\\mathrm {md}_p(\\vec{x}, \\vec{y})=\\left({\\frac  {1}{n}}\\sum _{ {i=1} }^{n}(x_{i}-y_{i})^{p}\\right)^{ { {\\frac  {1}{p}}}}\n",
    "\\end{align}\n",
    "\n",
    "Root Mean Squared Distance (RMSD):\n",
    "\\begin{align}\n",
    "\\mathrm {rmsd}(\\vec{x}, \\vec{y}) &= md_2(\\vec{x}, \\vec{y})\\\\ \n",
    "\\mathrm {rmsd}(\\vec{x}, \\vec{y}) &={\\sqrt { {\\frac {1}{n}}\\left(\\sum _{ {i=1} }^{n}(x_{i}-y_{i})^{2}\\right)}}\n",
    "\\end{align}\n",
    "\n",
    "$k$ Mean RMSD\n",
    "\\begin{align}\n",
    "\\mathrm {krmsd}(\\vec{x}, \\vec{y})=\\frac{\\sum_{i=1}^k \\mathrm{rmsd}(\\vec{x}, \\vec{y})}{k}\n",
    "\\end{align}\n",
    "### try:\n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} & = \\underset{y}{\\operatorname{argmin}}\\ P(y) \\prod _{ {i=1} }^{n}P(x_{i}\\vert y)\n",
    "\\end{align}\n",
    "\n",
    "I know, I'm breaking all the rules! \n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} & = \\underset{y}{\\operatorname{argmin}}\\ \\frac{P(y)\\prod _{ {i=1} }^{n}P(x_{i}\\vert y)}{1 - \\mathrm{krmsd}(\\vec{x}, \\vec{m_{iy}})} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB 0.931568144499\n",
      "KNN 0.971927663988\n",
      "KMC 0.968448059805\n",
      "PYKMC 0.968448059805\n",
      "MSEKMC 0.968448059805\n",
      "MSEKNN 0.971927663988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "ds = datasets.load_breast_cancer()\n",
    "ds.data = normalize(ds.data, axis=0)\n",
    "\n",
    "print \"GNB\",cross_validation.cross_val_score(GaussianNB(), ds.data, ds.target, cv=10).mean()\n",
    "print \"KNN\", cross_validation.cross_val_score(KNeighborsClassifier(), ds.data, ds.target, cv=10).mean()\n",
    "print \"KMC\", cross_validation.cross_val_score(KMC(), ds.data, ds.target, cv=10).mean()\n",
    "print \"PYKMC\", cross_validation.cross_val_score(PYKMC(), ds.data, ds.target, cv=10).mean()\n",
    "print \"MSEKMC\", cross_validation.cross_val_score(KMC(distance_function=mse_dist), ds.data, ds.target, cv=10).mean()\n",
    "print \"MSEKNN\", cross_validation.cross_val_score(MVKNN(distance_function=mse_dist), ds.data, ds.target, cv=10).mean()\n",
    "# print cross_validation.cross_val_score(MVKNN(), ds.data, ds.target, cv=10).mean()\n",
    "# print cross_validation.cross_val_score(KMC(), ds.data, ds.target, cv=10).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
