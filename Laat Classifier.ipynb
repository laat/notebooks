{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets.base import Bunch\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Throwing out a set of theorems and definitions\n",
    "## Bayes theorem:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|\\vec{x}) = \\frac{P(y)P(\\vec{x}|y)}{P(\\vec{x})}\n",
    "\\end{align}\n",
    "\n",
    "## Mean Squared Error\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname {MSE}={\\frac  {1}{n}}\\sum _{ {i=1} }^{n}({\\hat  {Y_{i}}}-Y_{i})^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier:\n",
    "\\begin{equation}\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}{P(x_1, \\dots, x_n)} \\\\\n",
    "\\end{equation}\n",
    "Using the naive independence assumption that\n",
    "\\begin{equation}\n",
    "P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)\n",
    "\\end{equation}\n",
    "simplify\n",
    "\\begin{equation}\n",
    "P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}\n",
    "\\end{equation}\n",
    "\n",
    "${P(x_1, \\dots, x_n)}$ is constant given the input.\n",
    "\n",
    "\\begin{align}\n",
    "P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y) \\\\\n",
    "\\Downarrow \\\\\n",
    "\\hat{y} = \\underset{y}{\\operatorname{argmax}}\\ P(y) \\prod_{i=1}^{n} P(x_i \\mid y) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Because float underflow, $log$ trick is often used\n",
    "\\begin{align}\n",
    "\\underset{y}{\\operatorname{argmax}}\\ P(y) \\prod_{i=1}^{n} P(x_i \\mid y)  & \\propto \\underset{y}{\\operatorname{argmax}}\\  log(P(y)) + \\sum _{ {i=1} }^{n} log(P(x_{i}\\vert y)) \\\\\n",
    " & = \\underset{y}{\\operatorname{argmax}}\\ - (log(P(y)) + \\sum _{ {i=1} }^{n} log(P(x_{i}\\vert y))) \\\\\n",
    " & = \\underset{y}{\\operatorname{argmin}}\\ - log(P(y)) - \\sum _{ {i=1} }^{n} log(P(x_{i}\\vert y)))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "euclid = lambda x,y: np.linalg.norm(x-y)\n",
    "\n",
    "def majority_vote_classify(distance_function, data, target, x, k=3):\n",
    "    distances = np.vectorize(distance_function)(data, x)\n",
    "    idx = np.argpartition(distances, k)\n",
    "    closest_classes = target[idx[:k]]\n",
    "    cl = np.bincount(closest_classes).argmax()\n",
    "    return cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Mean Classification Rule ($k$MC) for $k$NN:\n",
    "\n",
    "General\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &=\\underset{y}{\\operatorname{argmin}}\\ \\frac{\\sum_{i=1}^k \\text{dist}(\\vec{x}, m_{iy})}{k}\\\\\n",
    "\\end{align}\n",
    "k is independent of y, simplify\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &=\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\text{dist}(\\vec{x}, m_{iy})\\\\\n",
    "\\end{align}\n",
    "\n",
    "Manhattan\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &=\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|}\n",
    "\\end{align}\n",
    "\n",
    "With euclids distance\n",
    "\\begin{align}\n",
    "{\\hat  {y}}  &=\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(x_i - m_{ijy})^2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Laat Classifier\n",
    "\n",
    "I fantasize that $k$NN and NB are exactly the same. Can the classification rules be combined in an intuitive way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For funzies \n",
    "\n",
    "Consider Naive Bayes, $k$MC with manhattan and euclids distance:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ P(y) \\prod _{ {i=1} }^{n}P(x_{i}\\vert y) \\\\\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|} \\\\\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(x_i - m_{ijy})^2}}\n",
    "\\end{align}\n",
    "\n",
    "Observation:\n",
    "\n",
    "$k$NN does not have something that looks like $P(y)$, could the \"ball\" arround $\\vec{x}$ defined by $k$ be somewhat similar? \n",
    "\n",
    "If so, then \n",
    "\\begin{equation} \n",
    "\\prod _{ {i=1} }^{n}P(x_{i}\\vert y)\n",
    "\\end{equation}\n",
    "from naive bayes seems more interesting when combining the two concepts.\n",
    "\n",
    "\n",
    "If its not, then maybe using $P(y)$ in $k$NN somehow could yield better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using $P(y)$ in $k$NN\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ (1-P(y)) \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(x_i - m_{ijh})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "My gut feeling says that this probably has too much impact on estimation, and we would end up with classifying too many $\\hat{y}$ with the highest $P(y)$ especially in cases where differences in $P(y)$ is high \n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtracting likelihood from distance\n",
    "Just subtracting the likelyhood from the distance, would that yield better classification?\n",
    "Seems silly. \n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} =\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|} - P(x_{i}\\vert y)\n",
    "\\end{equation}\n",
    "\n",
    "Wait, negative distances is probably bad. Lets fix that....\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} =\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|} + (1-P(x_{i}\\vert y))\n",
    "\\end{equation}\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using $p(y)$ and subtracting likelihood from distance\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} =\\underset{y}{\\operatorname{argmin}}\\ (1-P(y)) \\sum_{i=1}^k \\sum_{j=1}^n{|(x_i - m_{ijy})|}+ (1-P(x_{i}\\vert y))\n",
    "\\end{equation}\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding imaginary likelyhood to $m_{ijy}$?\n",
    "\n",
    "Mapping $x$ to imaginary $x$-likelyhood:\n",
    "\\begin{equation}\n",
    "\\mathcal{I}(x|y) = x + P(x|y)i\n",
    "\\end{equation}\n",
    "\n",
    "Then\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(x_i - \\mathcal{I}(m_{ijy}|y))^2}}\n",
    "\\end{equation}\n",
    "\n",
    "Seems to me that this could \"clean\" the training data, unlikely features of $m_{iy}$ gives logner distances from the test vector $\\vec{x}$. It could even be computed once when training.\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding imaginary likelyhood to $x_i$?\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}(x_i|y) - m_{ijy})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "This should penalize features in $\\vec{x}$ that is unlikely to be a part of $y$\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding imaginary likelyhood to $x_i$ and $m_{ijy}$?\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}(x_i|y) - \\mathcal{I}(m_{ijy}|y))^2}}\n",
    "\\end{equation}\n",
    "\n",
    "Nah, this sort of negates the penalty for unlikely features in $y$. We can do better!\n",
    "\n",
    "Lets take the complex conjugate $\\overline{\\mathcal{I}( \\cdot, \\cdot )}$ of one of them, it shouldn't matter which.\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}(x_i|y) - \\overline {\\mathcal{I}(m_{ijy}|y)})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "This should penalize both unlikely features in $\\vec{x}$ and pushing noisy training data away from $\\vec{x}$\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using $P(y)$ *and* imaginary likelyhood\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat  {y}} = \\underset{y}{\\operatorname{argmin}}\\ (1-P(y)) \\sum_{i=1}^k \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}(x_i|y) - \\overline {\\mathcal{I}(m_{ijh}|y)})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "** TODO ** test on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is the distance a metric?\n",
    "\\begin{equation}\n",
    "d_{y}(\\vec{x}, \\vec{y}) = \\sqrt{\\sum_{j=1}^n{(\\mathcal{I}_{y}(x_i) - \\overline {\\mathcal{I}_{y}(y_i)})^2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "1. $d_{y}(x,y)\\geq 0$ non-negativity 👍\n",
    "2. $d_{y}(x,x) = 0$ identity 👎\n",
    "3. $d_{y}(x,y) = d_{y}(y,x) $ symmetry 👍\n",
    "4. $d_{y}(x,z) \\leq d_{y}(x,y) + d_{y}(y,z)$  triangle inequality ? 👎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE as distance?\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname {MSE}={\\frac  {1}{n}}\\sum _{ {i=1} }^{n}({\\hat  {Y_{i}}}-Y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} &= \\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k {\\frac  {1}{n}}\\sum _{ {i=1} }^{n}({x_{i}}-m_{ijy})^{2} \\\\\n",
    "&=\\underset{y}{\\operatorname{argmin}}\\ \\sum_{i=1}^k \\sum _{ {i=1} }^{n}({x_{i}}-m_{ijy})^{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Funzies\n",
    "\n",
    "### Requirement: \n",
    "\n",
    "All features must be normalized so that all $x$ in $X$ is between $[0,1]$\n",
    "\n",
    "### consider:\n",
    "\n",
    "Generalized Mean:\n",
    "\\begin{align}\n",
    "\\mathrm {m}_p(\\vec{x})=\\left({\\frac  {1}{n}}\\sum _{ {i=1} }^{n}x_{i}^{p}\\right)^{ { {\\frac  {1}{p}}}}\n",
    "\\end{align}\n",
    "\n",
    "Generalized Mean Distance:\n",
    "\\begin{align}\n",
    "\\mathrm {md}_p(\\vec{x}, \\vec{y})=\\left({\\frac  {1}{n}}\\sum _{ {i=1} }^{n}(x_{i}-y_{i})^{p}\\right)^{ { {\\frac  {1}{p}}}}\n",
    "\\end{align}\n",
    "\n",
    "Root Mean Squared Distance (RMSD):\n",
    "\\begin{align}\n",
    "\\mathrm {rmsd}(\\vec{x}, \\vec{y}) &= md_2(\\vec{x}, \\vec{y})\\\\ \n",
    "\\mathrm {rmsd}(\\vec{x}, \\vec{y}) &={\\sqrt { {\\frac {1}{n}}\\left(\\sum _{ {i=1} }^{n}(x_{i}-y_{i})^{2}\\right)}}\n",
    "\\end{align}\n",
    "\n",
    "$k$ Mean RMSD\n",
    "\\begin{align}\n",
    "\\mathrm {krmsd}(\\vec{x}, \\vec{y})=\\frac{\\sum_{i=1}^k \\mathrm{rmsd}(\\vec{x}, \\vec{y})}{k}\n",
    "\\end{align}\n",
    "### try:\n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} & = \\underset{y}{\\operatorname{argmin}}\\ P(y) \\prod _{ {i=1} }^{n}P(x_{i}\\vert y)\n",
    "\\end{align}\n",
    "\n",
    "I know, I'm breaking all the rules! \n",
    "\n",
    "\\begin{align}\n",
    "{\\hat  {y}} & = \\underset{y}{\\operatorname{argmin}}\\ \\frac{P(y)\\prod _{ {i=1} }^{n}P(x_{i}\\vert y)}{1 - \\mathrm{krmsd}(\\vec{x}, \\vec{m_{iy}})} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
